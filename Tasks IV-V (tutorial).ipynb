{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environmental Solutions Lab: Adaptive Infrastructure\n",
    "## Tasks IV & V: Predicting blackout events using machine learning\n",
    "Intelligent Earth Center for Doctoral Training <br>\n",
    "Friday 13th December 2024 <br>\n",
    "\n",
    "Instructors:\n",
    "* <alison.peard@ouce.ox.ac.uk><br>\n",
    "* <yu.mo@chch.ox.ac.uk>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "In this task, you will use machine learning models to predict blackout events based on storm characteristics, blackout data, and socio-economic factors.\n",
    "\n",
    "When we finish the tutorial, play around with the model. Maybe a different model will be better than XGBoost, or we have missed some useful predictors? Are there any other datasets we can use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment set up\n",
    "\n",
    "To begin, let's load all the packages we will need.\n",
    "\n",
    "We use the `scikit-learn` library, which provides many generic machine learning methods like grid searching and train-test splitting. We want to use an `XGboost` model, so we need to also import the `xgboost` library.\n",
    "\n",
    "We define some arguments at the start of our code: `SEED`, to ensure reproducibility and `NORMALISE` to select whether to normalise the data before feeding it into the model. Normalisation is not needed for tree-based methods like XGBoost, so we can leave this switched off for now.\n",
    "\n",
    "We will be predicting a binary outcome, so we instantiate the classified model from `xgboost`, `XGBClassifier`.\n",
    "\n",
    "We also set up a variable pointing to out data directory `WD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import multiprocessing\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pp as prettyprint\n",
    "\n",
    "SEED = 42\n",
    "NORMALISE = False\n",
    "XGBModel = xgb.XGBClassifier #  xgb.XGBRegressor for regression\n",
    "\n",
    "WD = os.path.join(os.getcwd(), \"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and process the data\n",
    "\n",
    "We will use the data we prepared in Tasks I-III. This should consist of clean storm characteristics from IBTrACS, blackout durations from the Blackout dataset, plus additional socioeconomic data. The pre-prepared file `blackout_with_socio.csv` provided, so we can just load this in.\n",
    "\n",
    "We want to predict electricity outages (derived from blackout data) from storm characteristics and socioeconomic factors.\n",
    "\n",
    "Begin by loading and visualising the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(WD, \"blackout_with_socio.csv\"))\n",
    "print(f\"Data has {len(df.columns)} columns and {len(df)} rows.\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 30 columns. The `duration` column represents blackout duration and the remaining 29 are predictors. All of these are numeric except for `income`, which has string categories, which `XGBoost` cannot automatically handle. We need to encode this in a numeric format.\n",
    "\n",
    "Two common techniques are _label encoding_ and _one-hot encoding_. Label encoding should only be used when there is a clear hierarchy between categories, e.g., high, medium, low. It assigns an ordinal value to each category corresponding to the hierarchy. One-hot encoding creates a binary column for each category.\n",
    "\n",
    "Since country income has a clear hierarchy, low, lower-middle, upper-middle, and high, we opt for label-encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical to ordinal\n",
    "income_factors = {'L': 0, 'LM': 1, 'UM': 2, 'H': 3}\n",
    "df['income'] = df['income'].map(income_factors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blackout duration variable takes integer values corresponding to the number of days of blackout experienced after an IBTrACs storm. This is a difficult prediction task. One way to simplify the problem, while still gaining useful information, is to set a value of interest and focus on predicting whether the blackout duration exceeds this value. Here, we will use the median duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert duration to binary variable\n",
    "median = df['duration'].median()\n",
    "df['duration'] = (df['duration'] > median).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, `duration` is a binary variable indicating whether the median blackout duration has been exceeded for each sample.\n",
    "\n",
    "With the data processing has been done, split the dataset into train and test sets. We choose a subset of the possible predictor values, the maximum wind speed, the storm speed, the distance from the pixel to the storm track, and the socioeconomic income at that location. (Note we use the `SEED` variable to ensure reproduciblilty.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split\n",
    "response = \"duration\"\n",
    "regressors = [\"windmax\", \"speed\", \"distToTrack\", \"income\"]\n",
    "df = df.loc[:, regressors + [response]].copy()\n",
    "\n",
    "# 90:10 split\n",
    "df = df[regressors + [response]]\n",
    "train, test = train_test_split(df, train_size=0.9, shuffle=True, random_state=SEED)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "\n",
    "XGBoost ([eXtreme Gradient Boosting](https://xgboost.readthedocs.io/en/stable/)) is an ensemble algorithm based-on the predictions of hundreds of decision trees.\n",
    "\n",
    "There are two main types of ensemble algorithm:\n",
    "\n",
    "* **bagging algorithms:** (or bootstrap aggregating) are based on drawing bootstrap samples from your training data. Random forests are a special case of bagging algorithm where the predictive features at each data split are also sub-sampled.\n",
    "\n",
    "* **boosting algorithms:** are iterative approaches, where an ensemble consists of a sequence of weak learners, each tuned to compensate for the weaknesses in their predecessors. XGBoost is a gradient boosting algorithm where the weak learners are hundreds or thousands of decision trees.\n",
    "\n",
    "XGBoost models have a large number of parameters relating to the base learners (decision trees) and the booster. These are described in detail in the [XGboost documentation](https://xgboost.readthedocs.io/en/stable/parameter.html).\n",
    "\n",
    "We will use `scikit-learn`'s `GridSearchCV` to do cross-validatory gridsearch through a subset of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(XGBModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "X, y = train[regressors], train[response]\n",
    "\n",
    "model = XGBModel(\n",
    "    n_jobs=multiprocessing.cpu_count() // 2,\n",
    "    tree_method=\"hist\",\n",
    "    objective=\"binary:logistic\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the grid search\n",
    "cv = GridSearchCV(\n",
    "    model,\n",
    "    {   \"booster\": [\"gbtree\"],            # default, could try \"gblinear\"\n",
    "        \"learning_rate\": [0.1, 1],        # alias eta ~[0.01 - 0.3], lower leads to slower computation\n",
    "        \"min_split_loss\": [1],            # alias gamma, regulation. ~[0,inf]. Start with 0 and check CV error rate. If you see train error >>> test error, bring gamma into action. Higher the gamma, lower the difference in train and test CV.\n",
    "        \"alpha\": [1],                     # L1 regularization (equivalent to Lasso regression) on weights.\n",
    "        \"lambda\": [1],                    # L2 regularization (equivalent to Ridge regression) on weights. It is used to avoid overfitting\n",
    "        \"max_depth\": [8],                 # the depth of the tree.Larger the depth, more complex the model; higher chances of overfitting\n",
    "        \"min_child_weight\": [6],\n",
    "        \"subsample\": [0.8, 0.9],          # number of samples (observations) supplied to a tree. ~ (0.5-0.8)\n",
    "        \"colsample_bytree\": [0.8, 0.9],   # the number of features (variables) supplied to a tree. [0,1] ~(0.5,0.9)\n",
    "        \"colsample_bylevel\": [0.8, 0.9],  # the number of features (variables) supplied to each level. [0,1] ~(0.5,0.9)\n",
    "        \"colsample_bynode\": [0.8, 0.9]    # the number of features (variables) supplied to each node. [0,1] ~(0.5,0.9)\n",
    "        },\n",
    "    verbose=1,\n",
    "    n_jobs=multiprocessing.cpu_count() // 2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform the cv\n",
    "cv.fit(X, y)\n",
    "\n",
    "prettyprint(cv.best_score_)\n",
    "prettyprint(cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the full CV results dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in cv_df.columns:print(col) # to see what columns are available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the gridsearch results as a DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df = pd.DataFrame(cv.cv_results_)\n",
    "param_cols = [col for col in cv_df.columns if col.startswith(\"param_\")]\n",
    "cv_df[[\"rank_test_score\", \"mean_test_score\", \"std_test_score\"] + param_cols].sort_values(by=\"rank_test_score\", ascending=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save parameters to a csv file \n",
    "best_params = pd.DataFrame.from_dict(cv.best_params_, orient=\"index\")\n",
    "best_params.columns = [\"value\"]\n",
    "best_params.to_csv(os.path.join(WD, \"best_params.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the best model\n",
    "\n",
    "Now that we have finished hyperparameter tuning, we can fit the model with the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = pd.read_csv(os.path.join(WD, \"best_params.csv\"), index_col=0)\n",
    "best_params = best_params.to_dict()[\"value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = XGBModel(\n",
    "    n_jobs=multiprocessing.cpu_count() // 2,\n",
    "    tree_method=\"hist\",\n",
    "    **best_params\n",
    ")\n",
    "best_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at results\n",
    "\n",
    "Get fits and predictions by applying the fitted XGBoost model to the train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train[response]\n",
    "y_test = test[response]\n",
    "y_fit = best_model.predict(train[regressors])\n",
    "y_pred = best_model.predict(test[regressors])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a binary problem, accuracy alone does not give us all the information. We calculate the confusion (contingency) matrix of true positives, true negatives, false positives, and false negatives and look at some popular derived metrics.\n",
    "\n",
    "* accuracy: overall proportion correct\n",
    "* bias: balance between under- and over-prediction (1 indicates unbiased)\n",
    "* precision / specificity: are we avoiding all the negatives?\n",
    "* recall / sensitivity: are we catching all the positives?\n",
    "* CSI balances precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification metrics\n",
    "confusion_matrix = pd.crosstab(y_test, y_pred, rownames=['Observation'], colnames=['Prediction'])\n",
    "a = confusion_matrix[1][1] # true positives\n",
    "b = confusion_matrix[0][1] # false positives\n",
    "c = confusion_matrix[1][0] # false negatives\n",
    "d = confusion_matrix[0][0] # true negatives\n",
    "\n",
    "accuracy = np.mean(y_test == y_pred)\n",
    "\n",
    "bias = (a + b) / (a + c)\n",
    "precision = a / (a + b)\n",
    "recall = a / (a + c)\n",
    "csi = a / (a + b + c) # critical success index (f2)\n",
    "\n",
    "\n",
    "print('Results:\\n--------')\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Bias: {bias:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'CSI: {csi:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance and SHAP\n",
    "\n",
    "XGBoost's built-in feature importance is based-on summing up the feature importances for all the decision trees in the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at built-in feature importances (not recommended)\n",
    "rank = best_model.feature_importances_.argsort()\n",
    "plt.barh(df.columns[rank], best_model.feature_importances_[rank])\n",
    "plt.xlabel('Default XGBoost feature importances')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Afternoon assignment\n",
    "\n",
    "For the rest of the tutorial, split into  groups of 3--4 to explore the model, predictors, and hyperparameters more. At the end of the tutorial we will ask each group to present three slides and share what you've found.\n",
    "\n",
    "#### Three slides\n",
    "\n",
    "1.\tHyperparameter importance: identify the most relevant XGBoost hyperparameters for model tuning.\n",
    "2.\tPredictor importance: identify the most important predictors. Compare different variable importance metrics. Summarise key insights, focusing on the storm characteristics and socio-economic factors that most influence power outages. Discuss any patterns or trends identified in the data. \n",
    "3.\tReport your best scores(s). Try other models/parameters/hyperparameters. Feel free to go more complex or more simple.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Extra) Shapely values with SHAP\n",
    "\n",
    "SHAP is now widely used to determine feature importance and can provide a more information about how models make predictions. SHAP values are model-agnostic and generally reliable. Roughly, they calculate the expected marginal contribution of each variable across all samples. \n",
    "\n",
    "We use the python `shap` library. You can read the docs [here](https://shap.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs() # for plotting\n",
    "\n",
    "explainer = shap.Explainer(best_model)\n",
    "shap_values = explainer(train[regressors])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a plot similar to the feature importance plot by plotting the average SHAP value across all samples. The results are similar to those of XGBoost's feature importance, except speed and windmax are swapped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $x$-axis shows the average absolute contribution it made to the outcome variable, in this case the log-odds of a blackout exceeding the median duration.\n",
    "\n",
    "We can also look at the SHAP values for individual observations using a waterfall plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# waterfall\n",
    "shap.plots.waterfall(shap_values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sample zero, the maximum wind speed reduced the probability of a long blackout.\n",
    "\n",
    "We can also view this as a force plot, which is basically a condensed waterfall plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# force plot\n",
    "shap.plots.force(shap_values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A stacked force plot allows us to visualise the SHAP values of multiple variables at once. This is made by rotating the force plots to vertical for each observation, and interpolating between them to create smooth lines. Note the plot is interactive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# stacked force plot\n",
    "shap.plots.force(shap_values[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beeswarm plot provide an information-dense summary of how the top features impact the model's output. The $x$-axis position indicates the SHAP value of a predictor for each sample, while the shading indicates whether the predictor had a low or high value for that sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beeswarm\n",
    "shap.plots.beeswarm(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the relationship between the SHAP values and one or more predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.scatter(shap_values[:, \"windmax\"], color=shap_values[:, \"speed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "1. Stephens (2013) Problems with binary pattern measures for flood model evaluation\n",
    "2. https://shap.readthedocs.io/en/latest/\n",
    "3. https://xgboost.readthedocs.io/en/stable/python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adaptive-infra-env",
   "language": "python",
   "name": "adaptive-infra-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
