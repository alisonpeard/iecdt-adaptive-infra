{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environmental Solutions Lab: Adaptive Infrastructure\n",
    "## Tasks IV & V: Predicting blackout events using machine learning\n",
    "Intelligent Earth Center for Doctoral Training <br>\n",
    "Friday 13th December 2024 <br>\n",
    "\n",
    "Instructors:\n",
    "* <alison.peard@ouce.ox.ac.uk><br>\n",
    "* <yu.mo@chch.ox.ac.uk>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "In this task, you will use machine learning models to predict blackout events based on storm characteristics, blackout data, and socio-economic factors.\n",
    "\n",
    "When we finish the tutorial, play around with the model. Maybe a different model will be better than XGBoost, or we have missed some useful predictors? Are there any other datasets we can use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment set up\n",
    "To begin, let's set up our environment.\n",
    "\n",
    "We will use the `scikit-learn` library for some generic machine learning methods like grid searching and train-test spits. We want to use an `XGboost` model, so we need to import this library seperately.\n",
    "\n",
    "We define some arguments at the start of our code: `SEED`, to ensure reproducibility and `NORMALISE` to select whether to normalise the data before feeding it into the model. Normalisation is not needed for tree-based methods, like XGBoost, so we can leave this switched off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import multiprocessing\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pp as prettyprint\n",
    "\n",
    "SEED = 42\n",
    "NORMALISE = False\n",
    "XGBModel = xgb.XGBClassifier #  xgb.XGBRegressor for regression\n",
    "\n",
    "WD = os.path.join(os.getcwd(), \"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and process the data\n",
    "We will use the data we prepared in Tasks I-III. This should consist of clean IBTrACs storm characteristics, blackout durations from the Blackout dataset, and extra socioeconomic data. There is a pre-prepared file `blackout_with_socio.csv` provided, so we can just load this in.\n",
    "\n",
    "We want to see if we can predict electricity outages (derived from blackout data) based on storm characteristics and socioeconomic factors. We begin by loading the data and visualising it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(os.path.join(WD, \"outages.csv\"))\n",
    "df = pd.read_csv(os.path.join(WD, \"blackout_with_socio.csv\"))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The income column is in categorical string format, which `XGBoost` cannot automatically handle. We need to encode this in a numeric format.\n",
    "\n",
    "Two common techniques are _label encoding_ and _one-hot encoding_. Label encoding should only be used when there is a clear hierarchy between categories, e.g., high, medium, low. It assigns an ordinal value to each category corresponding to the hierarchy. One-hot encoding creates a binary column for each category.\n",
    "\n",
    "Since country income has a clear hierarchy, low, lower-middle, upper-middle, and high, we can use label-encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical to ordinal\n",
    "income_factors = {'L': 0, 'LM': 1, 'UM': 2, 'H': 3}\n",
    "df['income'] = df['income'].map(income_factors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blackout duration variable takes integer values corresponding to the number of days of blackout experienced after an IBTrACs storm. This is a difficult prediction task. One way to simplify this task, while still gaining useful information, is to set a value of interest and focus on predicting whether the blackout duration exceeds this value. Here, we will use the median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert duration to binary variable\n",
    "median = df['duration'].median()\n",
    "df['duration'] = (df['duration'] > median).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all the data processing has been done, we split the dataset into train and test sets and view it once more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split\n",
    "response = \"duration\"\n",
    "regressors = [\"windmax\", \"speed\", \"distToTrack\", \"income\"]\n",
    "df = df.loc[:, regressors + [response]].copy()\n",
    "\n",
    "# 90:10 split\n",
    "df = df[regressors + [response]]\n",
    "train, test = train_test_split(df, train_size=0.9, shuffle=True, random_state=SEED)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGboost\n",
    "XGBoost ([eXtreme Gradient Boosting](https://xgboost.readthedocs.io/en/stable/)) is an ensemble algorithm based-on the predictions of hundreds of decision trees. There are two main types of ensemble algorithm:\n",
    "* **bagging:** (or bootstrap aggregating) is based on drawing bootstrap samples from your training step. Random forests are a special case of bagging algorithm where the predictive features at each data split are also subset.\n",
    "* **boosting:** is an iterative approach, where an ensemble consists of a sequence of weak learners, each tuned to compensate for the weaknesses in their predecessors. XGBoost is a gradient boosting algorithm where the weak learners are hundreds or thousands of decision trees.\n",
    "\n",
    "XGBoost models have a large number of parameters relating to the base learners (decision trees) and the booster. These are described in detail in the [XGboost documentation](https://xgboost.readthedocs.io/en/stable/parameter.html). We will use `scikit-learn`'s `GridSearchCV` to do cross-validatory gridsearch through a subset of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "X, y = train[regressors], train[response]\n",
    "model = XGBModel(\n",
    "    n_jobs=multiprocessing.cpu_count() // 2,\n",
    "    tree_method=\"hist\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the grid search\n",
    "cv = GridSearchCV(\n",
    "    model,\n",
    "    {   \"booster\": [\"gbtree\"],            # default\n",
    "        \"learning_rate\": [0.1, 1],        # alias eta\n",
    "        \"min_split_loss\": [1],            # alias gamma\n",
    "        \"alpha\": [1],\n",
    "        \"lambda\": [1],\n",
    "        \"max_depth\": [8],\n",
    "        \"min_child_weight\": [1],\n",
    "        \"subsample\": [0.8, 0.9],\n",
    "        \"colsample_bytree\": [0.8, 0.9],\n",
    "        \"colsample_bylevel\": [0.8, 0.9],\n",
    "        \"colsample_bynode\": [0.8, 0.9]\n",
    "        },\n",
    "    verbose=1,\n",
    "    n_jobs=multiprocessing.cpu_count() // 2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform the cv\n",
    "cv.fit(X, y)\n",
    "\n",
    "prettyprint(cv.best_score_)\n",
    "prettyprint(cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save parameters to a csv file \n",
    "best_params = pd.DataFrame.from_dict(cv.best_params_, orient=\"index\")\n",
    "best_params.columns = [\"value\"]\n",
    "best_params.to_csv(\"best_params.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = pd.read_csv(os.path.join(WD, \"best_params.csv\"), index_col=0)\n",
    "best_params = best_params.to_dict()[\"value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = XGBModel(\n",
    "    n_jobs=multiprocessing.cpu_count() // 2,\n",
    "    tree_method=\"hist\",\n",
    "    **best_params\n",
    ")\n",
    "best_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train[response]\n",
    "y_test = test[response]\n",
    "y_fit = best_model.predict(train[regressors])\n",
    "y_pred = best_model.predict(test[regressors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification metrics\n",
    "confusion_matrix = pd.crosstab(y_test, y_pred, rownames=['Observation'], colnames=['Prediction'])\n",
    "a = confusion_matrix[1][1] # true positives\n",
    "b = confusion_matrix[0][1] # false positives\n",
    "c = confusion_matrix[1][0] # false negatives\n",
    "d = confusion_matrix[0][0] # true negatives\n",
    "\n",
    "accuracy = np.mean(y_test == y_pred)\n",
    "\n",
    "bias = (a + b) / (a + c)\n",
    "f1 = (a + d) / (a + b + c + d)\n",
    "precision = a / (a + b)\n",
    "recall = a / (a + c)\n",
    "csi = a / (a + b + c) # critical success index (f2)\n",
    "\n",
    "\n",
    "print('Results:\\n--------')\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Bias: {bias:.2f}')\n",
    "print(f'F1: {f1:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'CSI: {csi:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance\n",
    "\n",
    "XGBoost's built-in feature importance is based-on summing up the feature importances for all the decision trees in the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at built-in feature importances (not recommended)\n",
    "rank = best_model.feature_importances_.argsort()\n",
    "plt.barh(df.columns[rank], best_model.feature_importances_[rank])\n",
    "plt.xlabel('Default XGBoost feature importances')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many practitioners prefer to use SHAP values to determine feature importance. SHAP values are model agnostic and generally considered to be more reliable. We use the python `shap` library.\n",
    "\n",
    "**Note:** for Apple Silicon Macs, `shap` needs to be installed with pip, not conda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "\n",
    "explainer = shap.TreeExplainer(best_model)\n",
    "shap_values = explainer.shap_values(test[regressors])\n",
    "shap.summary_plot(shap_values, test[regressors], plot_type=\"bar\", class_names=list(range(31)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "1. Stephens (2013) Problems with binary pattern measures for flood model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iecdt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
