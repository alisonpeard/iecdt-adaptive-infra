{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environmental Solutions Lab: Adaptive Infrastructure\n",
    "## Tasks I - III: Data preparation\n",
    "Intelligent Earth Center for Doctoral Training <br>\n",
    "Friday 13th December 2024 <br>\n",
    "\n",
    "Instructors:\n",
    "* <alison.peard@ouce.ox.ac.uk><br>\n",
    "* <yu.mo@chch.ox.ac.uk>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "WD = os.path.join(os.getcwd(), \"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Dataset | Download URL |\n",
    "| ------- | ------------ |\n",
    "| IBTrACs | https://www.ncei.noaa.gov/products/international-best-track-archive |\n",
    "| IBTrACs metadata (a) | https://climada-python.readthedocs.io/en/stable/_modules/climada/hazard/tc_tracks.html |\n",
    "| IBTrACs metadata (b) | https://doi.org/10.1175/2009BAMS2755.1 |\n",
    "| Blackout | https://blackmarble.gsfc.nasa.gov |\n",
    "| Natural Earth Cultural| https://www.naturalearthdata.com/downloads/10m-cultural-vectors |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task I: Clean IBTrACS data for storm characteristics input\n",
    "The IBTrACS (International Best Track Archive for Climate Stewardship) is a comprehensive dataset for tropical cyclones. You will clean and format this dataset for input into predictive models. The steps include:\n",
    "* Formatting: Review the source file for inconsistencies or formatting errors. Ensure the data is in a consistent and analysable format.\n",
    "* Standardizing Records Across Agencies: IBTrACS consolidates data from multiple agencies. Harmonize these variables to ensure uniformity, such as converting all wind speed measurements to the same standard.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IBTRACS = \"ibtracs.since1980.list.v04r00.csv\"\n",
    "AGENCIES = \"wind_factors.csv\"\n",
    "\n",
    "COLUMNS_IN = [\"SID\", \"SEASON\", \"NUMBER\",\"BASIN\",\"NAME\",\"NATURE\",\"ISO_TIME\",\n",
    "           \"LAT\",\"LON\",\"WMO_WIND\",\"WMO_PRES\",\"WMO_AGENCY\",\n",
    "           \"DIST2LAND\",\"LANDFALL\",'USA_WIND','USA_RMW','REUNION_WIND','REUNION_RMW',\n",
    "           'BOM_WIND',\"BOM_RMW\",\"STORM_SPEED\"]\n",
    "\n",
    "COLUMNS_OUT = [\"SID\",\"SEASON\",\"NUMBER\",\"BASIN\",\"NAME\",\"NATURE\",\"ISO_TIME\",\n",
    "               \"LAT\",\"LON\",\"WMO_WIND\",\"WMO_PRES\",\"STORM_SPEED\",\"DIST2LAND\",\n",
    "               \"LANDFALL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "ibtracs = pd.read_csv(os.path.join(WD, IBTRACS), \n",
    "skiprows=[1]) # second row is units\n",
    "ibtracs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check longitude range\n",
    "print(\"Longitude range:\", ibtracs['LON'].min(), ibtracs['LON'].max()) # ibtracs['LON'].max()\n",
    "print(\"Latitude range:\", ibtracs['LAT'].min(), ibtracs['LAT'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The longitude range looks strange, longitude is usually in (-180, 180] or [0, 360] while latitude is usually in [-90, 90] or [0, 180]. We solve this by simply subtracting 360 from any longitudes greater than 180."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_longitude(lon):\n",
    "    if lon > 180:\n",
    "        return lon - 360\n",
    "    return lon\n",
    "\n",
    "ibtracs['LON'] = ibtracs['LON'].apply(fix_longitude)\n",
    "print(\"Longitude range:\", ibtracs['LON'].min(), ibtracs['LON'].max()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are some blank rows in WMO_WIND\n",
    "ibtracs = ibtracs[ibtracs[\"WMO_WIND\"] != ' '].copy()\n",
    "ibtracs['WMO_WIND'] = ibtracs['WMO_WIND'].astype(float)\n",
    "ibtracs = ibtracs[COLUMNS_IN].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The World Meteorolical Organisation (WMO) defines the _maximum sustained wind_ as the 10-minute average wind speed 10m above ground level ($U_{10}$).\n",
    "\n",
    "Despite this, many organisations average over one-minute or three-minute periods. [Knapp et al (2010)](https://journals.ametsoc.org/view/journals/mwre/138/4/2009mwr3123.1.xml) provide a table of shift and scale factors to convert different agencies to the standard 10-minute maximum sustained wind.\n",
    "\n",
    "We will use a dataset of shift and scale factors to standardise our wind records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_factors = pd.read_csv(os.path.join(WD, AGENCIES), index_col=0)\n",
    "wind_factors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_winds(row:pd.Series) -> float:\n",
    "    agency = row['WMO_AGENCY']\n",
    "    scale = wind_factors.loc[agency, 'scale']\n",
    "    shift = wind_factors.loc[agency, 'shift']\n",
    "    new_wind = row['WMO_WIND'] * scale + shift\n",
    "    return new_wind\n",
    "\n",
    "ibtracs['WMO_WIND'] = ibtracs.apply(convert_winds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ibtracs[COLUMNS_OUT].to_csv(os.path.join(WD, \"ibtracs_formatted\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task II: Get storm information for blackout events\n",
    "The blackout dataset, derived from sources like the Black Marble project, captures key details about power outages, including duration, affected customers, and geographic locations. To integrate storm-related information into this dataset, complete the following steps:\n",
    "* Matching records: match the IBTrACS data with the blackout data by a common identifier, i.e., storm ID. \n",
    "* Extract storm information: For each matched blackout event, extract relevant storm characteristics from the IBTrACS dataset, such as wind speed, pressure, and speed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLACKOUT = \"blackout.csv\"\n",
    "IBTRACS = \"ibtracs_formatted\"\n",
    "JOINCOLS = [\"NAME\", \"SEASON\", \"WMO_WIND\", \"STORM_SPEED\", \"LON\", \"LAT\"]\n",
    "\n",
    "blackout = pd.read_csv(os.path.join(WD, BLACKOUT))\n",
    "ibtracs = pd.read_csv(os.path.join(WD, IBTRACS))\n",
    "ibtracs = ibtracs.set_index(\"SID\")\n",
    "blackout.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process times from sid column\n",
    "blackout['landing_time'] = blackout['sid'].str.split(\"_\").str[1]\n",
    "blackout['landing_year'] = blackout['landing_time'].str[:4]\n",
    "blackout['landing_month'] = blackout['landing_time'].str[4:6]\n",
    "blackout['landing_day'] = blackout['landing_time'].str[6:8]\n",
    "blackout['landing_hour'] = blackout['landing_time'].str[8:10]\n",
    "blackout.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now match based-on SID and extract name, season\n",
    "blackout['SID'] = blackout['sid'].str.split(\"_\").str[0]\n",
    "blackout = blackout.set_index(\"SID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blackout = blackout.join(ibtracs[JOINCOLS])\n",
    "blackout = blackout.rename(columns={\"NAME\": \"storm\", \"SEASON\": \"season\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "There are a number of ways to calculate the distances between two geospatial locations. The simplest is to use the `haversine` library. If this is not available, the haversine distance can be computed manually. This can also be achieved with `geopandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distances_haversine(blackout):\n",
    "    from haversine import haversine\n",
    "\n",
    "    def haversine_dist(coords):\n",
    "        lat1, lon1, lat2, lon2 = coords\n",
    "        return haversine((lat1, lon1), (lat2, lon2), unit='km')\n",
    "    \n",
    "    blackout = blackout.copy()\n",
    "    blackout['distToTrack'] = blackout[['lat', 'lon', 'LAT', 'LON']].apply(haversine_dist, axis=1)\n",
    "    blackout['distToTrack'] = blackout['distToTrack'] * 1000 # convert to meters\n",
    "    return blackout\n",
    "\n",
    "\n",
    "def calculate_distances_manual(blackout):\n",
    "    from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "    def haversine_dist(coords):\n",
    "        \"\"\"https://stackoverflow.com/questions/4913349/haversine-formula-in-python-bearing-and-distance-between-two-gps-points\"\"\"\n",
    "        lat1, lon1, lat2, lon2 = coords\n",
    "        R = 6372.8 # this is in km.  For Earth radius in kilometers use 3959.87433 miles\n",
    "\n",
    "        dLat = radians(lat2 - lat1)\n",
    "        dLon = radians(lon2 - lon1)\n",
    "        lat1 = radians(lat1)\n",
    "        lat2 = radians(lat2)\n",
    "\n",
    "        a = sin(dLat/2)**2 + cos(lat1)*cos(lat2)*sin(dLon/2)**2\n",
    "        c = 2*asin(sqrt(a))\n",
    "\n",
    "        return R * c\n",
    "    \n",
    "    blackout = blackout.copy()\n",
    "    blackout['distToTrack'] = blackout[['lat', 'lon', 'LAT', 'LON']].apply(haversine_dist, axis=1)\n",
    "    blackout['distToTrack'] = blackout['distToTrack'] * 1000 # convert to meters\n",
    "    return blackout\n",
    "\n",
    "\n",
    "def calculate_distances_with_geopandas(blackout):\n",
    "    import geopandas as gpd\n",
    "\n",
    "    columns = list(blackout.columns)\n",
    "    blackout = gpd.GeoDataFrame(blackout)\n",
    "    blackout['points_blackout'] = gpd.GeoSeries.from_xy(blackout['lon'], blackout['lat']).set_crs(4326)\n",
    "    blackout['points_ibtracs'] = gpd.GeoSeries.from_xy(blackout['LON'], blackout['LAT']).set_crs(4326)\n",
    "\n",
    "    # switch to Pseudo Mercator (in metres)\n",
    "    blackout['points_blackout'] = blackout['points_blackout'].to_crs(3857)\n",
    "    blackout['points_ibtracs'] = blackout['points_ibtracs'].to_crs(3857)\n",
    "\n",
    "    blackout['distToTrack'] = blackout['points_blackout'].distance(blackout['points_ibtracs'])\n",
    "    blackout['distToTrack'] = blackout['distToTrack']\n",
    "\n",
    "    # geopandas doesn't handle NaNs well\n",
    "    nan_mask = blackout[['lat', 'lon', 'LAT', 'LON']].isnull().any(axis=1)\n",
    "    blackout.loc[nan_mask, 'distToTrack'] = pd.NA\n",
    "\n",
    "    return blackout[columns + ['distToTrack']]\n",
    "\n",
    "# play around with these\n",
    "distance_func = calculate_distances_with_geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get distances\n",
    "blackout = distance_func(blackout)\n",
    "blackout['distToTrack'].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate over storm: maxwind, mean speed, min dist to track\n",
    "storm_characteristics = blackout.groupby(\"SID\").agg(\n",
    "    windmax=(\"WMO_WIND\", \"max\"),\n",
    "    speed=(\"STORM_SPEED\", \"mean\"),\n",
    "    distToTrack=(\"distToTrack\", \"min\")\n",
    ")\n",
    "\n",
    "storm_characteristics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blackout[['windmax', 'speed', 'distToTrack']] = storm_characteristics\n",
    "blackout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save processed blackout data\n",
    "blackout = blackout.dropna(subset=['storm'])\n",
    "blackout.to_csv(os.path.join(WD, \"blackout_with_storms.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task III: Match socioeconomic data with blackout data\n",
    "Socio-economic factors such as population density, income, and urbanization levels are critical for understanding the context of power outages. To incorporate this information into the dataset, follow these steps:\n",
    "* Geographic matching: Use location identifiers, i.e., latitude and longitude, to link socio-economic data with the blackout dataset.\n",
    "* Handle missing socio-economic data: Identify missing socio-economic values in the matched dataset. Apply approximation techniques to fill in the empty data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the blackout data as a geospatial dataframe this time\n",
    "blackout = pd.read_csv(os.path.join(WD, \"blackout_with_storms.csv\"))\n",
    "blackout = gpd.GeoDataFrame(blackout, geometry=gpd.points_from_xy(blackout.lon, blackout.lat))\n",
    "blackout = blackout.set_crs(4326)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Socioeconomic data\n",
    "Versions of geopandas ≤ 1.0 have a `geopandas.datasets` module which allows us to import Natural Earth socioeconomic data directly to Python. Newer versions [have deprecated this module](https://github.com/geopandas/geopandas/issues/2751) but the country-level data (Admin 0 - Countries) can be downloaded from the [Natural Earth downloads page](https://www.naturalearthdata.com/downloads/10m-cultural-vectors/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 1\n",
    "SOCIOFILE = \"ne_110m_admin_0_countries\"\n",
    "SOCIOCOLS = ['pop_est', 'continent', 'name', 'iso3', 'gdp_md_est', 'income', 'geometry']\n",
    "\n",
    "socio = gpd.read_file(os.path.join(WD, SOCIOFILE))\n",
    "print(list(socio.columns))\n",
    "print(list(socio['INCOME_GRP'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give the income groups simpler names\n",
    "income_groups = {1: 'H', 2: 'H', 3: 'UM', 4: 'LM', 5: 'L'}\n",
    "socio['INCOME_GRP'] = socio['INCOME_GRP'].str.split('.').str[0].astype(int)\n",
    "socio['INCOME_GRP'] = socio['INCOME_GRP'].map(income_groups)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 1\n",
    "rename_columns = {'NAME': 'name', 'CONTINENT': 'continent', 'POP_EST': 'pop_est',\n",
    "                  'GDP_MD': 'gdp_md_est', 'INCOME_GRP': 'income', 'ISO_A3': 'iso3'}\n",
    "\n",
    "socio = socio.rename(columns=rename_columns)\n",
    "socio = socio[SOCIOCOLS].copy()\n",
    "socio.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for rows with missing values\n",
    "socio[socio.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will do a spatial join, to match each blackout location to a country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good practice to check crs alignment before geospatial ops\n",
    "assert socio.crs == blackout.crs\n",
    "\n",
    "if False: # make True to visualise, a little slow\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    socio.boundary.plot(ax=ax, color='black')\n",
    "    blackout.plot(ax=ax, color='red', markersize=5)\n",
    "    fig.suptitle('Blackout locations and country boundaries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the spatial join and save\n",
    "blackout = gpd.sjoin(blackout, socio, how=\"left\", predicate='within')\n",
    "blackout = blackout.drop(columns=['index_right', 'geometry'])\n",
    "\n",
    "blackout.to_csv(os.path.join(WD, \"blackout_with_socio.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix: Wind factors\n",
    "> Scale and shift used by agencies to convert their internal Dvorak 1-minute sustained winds to the officially reported values that are in IBTrACS. From Table 1 in  Knapp, K.R. & Kruk, M.C. (2010): Quantifying Interagency Differences in Tropical Cyclone Best-Track Wind Speed Estimates. Monthly Weather Review 138(4): 1459–1473. https://journals.ametsoc.org/view/journals/mwre/138/4/2009mwr3123.1.xml \n",
    "\n",
    "https://climada-python.readthedocs.io/en/stable/_modules/climada/hazard/tc_tracks.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IBTRACS_AGENCY_1MIN_WIND_FACTOR = {\n",
    "    \"usa\": [1.0, 0.0],\n",
    "    \"tokyo\": [0.60, 23.3],\n",
    "    \"newdelhi\": [1.0, 0.0],\n",
    "    \"reunion\": [0.88, 0.0],\n",
    "    \"bom\": [0.88, 0.0],\n",
    "    \"nadi\": [0.88, 0.0],\n",
    "    \"wellington\": [0.88, 0.0],\n",
    "    'cma': [0.871, 0.0],\n",
    "    'hko': [0.9, 0.0],\n",
    "    'ds824': [1.0, 0.0],\n",
    "    'td9636': [1.0, 0.0],\n",
    "    'td9635': [1.0, 0.0],\n",
    "    'neumann': [0.88, 0.0],\n",
    "    'mlc': [1.0, 0.0],\n",
    "}\n",
    "\"\"\"\"\"\"\n",
    "\n",
    "\n",
    "IBTRACS_USA_AGENCIES = [\n",
    "    'atcf', 'cphc', 'hurdat_atl', 'hurdat_epa', 'jtwc_cp', 'jtwc_ep', 'jtwc_io',\n",
    "    'jtwc_sh', 'jtwc_wp', 'nhc_working_bt', 'tcvightals', 'tcvitals'\n",
    "]\n",
    "\n",
    "IBTRACS_USA_1MIN_WIND_FACTOR = {\n",
    "    agency: IBTRACS_AGENCY_1MIN_WIND_FACTOR['usa'] for agency in IBTRACS_USA_AGENCIES\n",
    "    }\n",
    "WIND_FACTORS = IBTRACS_AGENCY_1MIN_WIND_FACTOR | IBTRACS_USA_1MIN_WIND_FACTOR\n",
    "\n",
    "wind_factors = pd.DataFrame(WIND_FACTORS).T\n",
    "wind_factors.columns = [\"scale\", \"shift\"]\n",
    "wind_factors.index.name = \"agency\"\n",
    "wind_factors.to_csv(os.path.join(WD, \"wind_factors.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adaptive-infra-env",
   "language": "python",
   "name": "adaptive-infra-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
